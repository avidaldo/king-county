{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Preprocessing: Step-by-Step\n",
    "\n",
    "This notebook walks through the **complete preprocessing workflow** for the King County house price dataset, explaining each step in detail.\n",
    "\n",
    "**Note:** For production use, see the companion notebook `04b-preprocessing-pipeline.ipynb` which encapsulates all these steps in a reusable sklearn pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (21613, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00         1180   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25         2570   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00          770   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00         1960   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00         1680   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view  ...  grade  sqft_above  sqft_basement  \\\n",
       "0      5650     1.0           0     0  ...      7        1180              0   \n",
       "1      7242     2.0           0     0  ...      7        2170            400   \n",
       "2     10000     1.0           0     0  ...      6         770              0   \n",
       "3      5000     1.0           0     0  ...      7        1050            910   \n",
       "4      8080     1.0           0     0  ...      8        1680              0   \n",
       "\n",
       "   yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \\\n",
       "0      1955             0    98178  47.5112 -122.257           1340   \n",
       "1      1951          1991    98125  47.7210 -122.319           1690   \n",
       "2      1933             0    98028  47.7379 -122.233           2720   \n",
       "3      1965             0    98136  47.5208 -122.393           1360   \n",
       "4      1987             0    98074  47.6168 -122.045           1800   \n",
       "\n",
       "   sqft_lot15  \n",
       "0        5650  \n",
       "1        7639  \n",
       "2        8062  \n",
       "3        5000  \n",
       "4        7503  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"harlfoxem/housesalesprediction\")\n",
    "csv_path = Path(path) / \"kc_house_data.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2. Parse Dates\n",
    "\n",
    "The `date` column contains sale dates as strings like `\"20140502T000000\"`. We need to parse these into proper datetime objects.\n",
    "\n",
    "**⚠️ CRITICAL:** Date parsing must happen **BEFORE** splitting the data. Because we need temporal ordering to create train/val/test splits. The string format cannot be sorted chronologically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2014-05-02 to 2015-05-27\n",
      "Total sales period: 390 days\n"
     ]
    }
   ],
   "source": [
    "# Parse the date string to datetime\n",
    "# Format: \"YYYYMMDDTHHMMSS\" → we only need the first 8 characters (YYYYMMDD)\n",
    "df[\"date_parsed\"] = pd.to_datetime(df[\"date\"].str[:8], format=\"%Y%m%d\")\n",
    "\n",
    "# Sort by date for temporal ordering\n",
    "df = df.sort_values(\"date_parsed\").reset_index(drop=True)\n",
    "\n",
    "print(f\"Date range: {df['date_parsed'].min().date()} to {df['date_parsed'].max().date()}\")\n",
    "print(f\"Total sales period: {(df['date_parsed'].max() - df['date_parsed'].min()).days} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 3. Temporal Train/Validation/Test Split\n",
    "\n",
    "We use **temporal splitting** (not random) to prevent data leakage, as seen in previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_train_val_test_split(\n",
    "    df: pd.DataFrame, \n",
    "    date_column: str,\n",
    "    val_size: float = 0.15, \n",
    "    test_size: float = 0.15\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split data temporally into train, validation, and test sets.\n",
    "    \n",
    "    Data is sorted by date_column, then split into three consecutive chunks.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Data to split.\n",
    "    date_column : str\n",
    "        Name of the datetime column for temporal ordering.\n",
    "    val_size : float\n",
    "        Proportion for validation set (default 0.15).\n",
    "    test_size : float  \n",
    "        Proportion for test set (default 0.15).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple of (train_df, val_df, test_df)\n",
    "    \"\"\"\n",
    "    df_sorted = df.sort_values(date_column).reset_index(drop=True)\n",
    "    \n",
    "    n = len(df_sorted)\n",
    "    train_end = int(n * (1 - val_size - test_size))\n",
    "    val_end = int(n * (1 - test_size))\n",
    "    \n",
    "    train_df = df_sorted.iloc[:train_end].copy()\n",
    "    val_df = df_sorted.iloc[train_end:val_end].copy()\n",
    "    test_df = df_sorted.iloc[val_end:].copy()\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes:\n",
      "  Train: 15,129 records (70.0%)\n",
      "  Val:   3,242 records (15.0%)\n",
      "  Test:  3,242 records (15.0%)\n",
      "\n",
      "Date ranges:\n",
      "  Train: 2014-05-02 to 2015-01-16\n",
      "  Val:   2015-01-16 to 2015-03-26\n",
      "  Test:  2015-03-26 to 2015-05-27\n"
     ]
    }
   ],
   "source": [
    "# Apply the split\n",
    "train_df, val_df, test_df = temporal_train_val_test_split(df, date_column=\"date_parsed\")\n",
    "\n",
    "print(\"Split sizes:\")\n",
    "print(f\"  Train: {len(train_df):,} records ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Val:   {len(val_df):,} records ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Test:  {len(test_df):,} records ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "print()\n",
    "print(\"Date ranges:\")\n",
    "print(f\"  Train: {train_df['date_parsed'].min().date()} to {train_df['date_parsed'].max().date()}\")\n",
    "print(f\"  Val:   {val_df['date_parsed'].min().date()} to {val_df['date_parsed'].max().date()}\")\n",
    "print(f\"  Test:  {test_df['date_parsed'].min().date()} to {test_df['date_parsed'].max().date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Based on the EDA findings (`01-eda.ipynb`), we'll create several derived features that better capture the underlying patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### 4.1 Temporal Feature: `days_since_start`\n",
    "\n",
    "This feature captures **market trends over time** — real estate prices tend to appreciate (or depreciate) systematically.\n",
    "\n",
    "We compute: `days_since_start = sale_date - reference_date`\n",
    "\n",
    "> **⚠️ CRITICAL: Data Leakage Warning**\n",
    ">\n",
    "> The `reference_date` must be computed from the **training set ONLY** and then applied to all splits.\n",
    "> \n",
    "> **Wrong approach (causes data leakage):**\n",
    "> ```python\n",
    "> # DON'T DO THIS!\n",
    "> for split in [train_df, val_df, test_df]:\n",
    ">     min_date = split[\"date_parsed\"].min()  # Different per split!\n",
    ">     split[\"days_since_start\"] = (split[\"date_parsed\"] - min_date).dt.days\n",
    "> ```\n",
    "> \n",
    "> This causes the feature to have **inconsistent semantics** across splits:\n",
    "> - Training: `days_since_start=300` means \"late in training period\"\n",
    "> - Validation: `days_since_start=0` means \"start of validation\" (which is actually later!)\n",
    "> \n",
    "> The model learns that high values mean \"late\", but validation/test reset to 0!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference date (from training): 2014-05-02\n"
     ]
    }
   ],
   "source": [
    "# CORRECT: Compute reference date from TRAINING data only\n",
    "train_min_date = train_df[\"date_parsed\"].min()\n",
    "print(f\"Reference date (from training): {train_min_date.date()}\")\n",
    "\n",
    "# Store this value! It's needed for inference on new data.\n",
    "# In a production pipeline, this would be a fitted parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "days_since_start ranges (should increase across splits):\n",
      "  Train:   0 to 259\n",
      "  Val:   259 to 328\n",
      "  Test:  328 to 390\n"
     ]
    }
   ],
   "source": [
    "# Apply the SAME reference date to ALL splits\n",
    "train_df[\"days_since_start\"] = (train_df[\"date_parsed\"] - train_min_date).dt.days\n",
    "val_df[\"days_since_start\"] = (val_df[\"date_parsed\"] - train_min_date).dt.days\n",
    "test_df[\"days_since_start\"] = (test_df[\"date_parsed\"] - train_min_date).dt.days\n",
    "\n",
    "print(\"days_since_start ranges (should increase across splits):\")\n",
    "print(f\"  Train: {train_df['days_since_start'].min():3d} to {train_df['days_since_start'].max():3d}\")\n",
    "print(f\"  Val:   {val_df['days_since_start'].min():3d} to {val_df['days_since_start'].max():3d}\")\n",
    "print(f\"  Test:  {test_df['days_since_start'].min():3d} to {test_df['days_since_start'].max():3d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "**Verify:** The validation and test ranges should be **higher** than training (they're in the future). If you see values starting from 0 in each split, you have data leakage!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### 4.2 Age Features\n",
    "\n",
    "These features capture the property's age-related characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age features added: house_age, was_renovated, years_since_renovation\n"
     ]
    }
   ],
   "source": [
    "def add_age_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add age-related features (per-row computation, no fitting needed).\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    sale_year = df[\"date_parsed\"].dt.year\n",
    "    \n",
    "    # House age at time of sale\n",
    "    df[\"house_age\"] = sale_year - df[\"yr_built\"]\n",
    "    \n",
    "    # Was the house ever renovated?\n",
    "    df[\"was_renovated\"] = (df[\"yr_renovated\"] > 0).astype(int)\n",
    "    \n",
    "    # Years since last renovation (0 if never renovated)\n",
    "    df[\"years_since_renovation\"] = np.where(\n",
    "        df[\"yr_renovated\"] > 0,\n",
    "        sale_year - df[\"yr_renovated\"],\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply to all splits\n",
    "train_df = add_age_features(train_df)\n",
    "val_df = add_age_features(val_df)\n",
    "test_df = add_age_features(test_df)\n",
    "\n",
    "print(\"Age features added: house_age, was_renovated, years_since_renovation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### 4.3 Ratio Features\n",
    "\n",
    "These features compare properties to themselves or neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio features added: basement_ratio, living_vs_neighbors, lot_vs_neighbors\n"
     ]
    }
   ],
   "source": [
    "def add_ratio_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add ratio features (per-row computation, no fitting needed).\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # What fraction of living space is basement?\n",
    "    df[\"basement_ratio\"] = df[\"sqft_basement\"] / df[\"sqft_living\"].replace(0, 1)\n",
    "    \n",
    "    # How does living area compare to neighbors?\n",
    "    df[\"living_vs_neighbors\"] = df[\"sqft_living\"] / df[\"sqft_living15\"].replace(0, 1)\n",
    "    \n",
    "    # How does lot size compare to neighbors?\n",
    "    df[\"lot_vs_neighbors\"] = df[\"sqft_lot\"] / df[\"sqft_lot15\"].replace(0, 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply to all splits\n",
    "train_df = add_ratio_features(train_df)\n",
    "val_df = add_ratio_features(val_df)\n",
    "test_df = add_ratio_features(test_df)\n",
    "\n",
    "print(\"Ratio features added: basement_ratio, living_vs_neighbors, lot_vs_neighbors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 5. Drop Non-Predictive Columns\n",
    "\n",
    "Some columns should be removed because they:\n",
    "- Are identifiers (not predictive)\n",
    "- Have been replaced by engineered features\n",
    "- Cannot be used effectively (high cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after dropping: ['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'lat', 'long', 'sqft_living15', 'sqft_lot15', 'days_since_start', 'house_age', 'was_renovated', 'years_since_renovation', 'basement_ratio', 'living_vs_neighbors', 'lot_vs_neighbors']\n",
      "Shape: (15129, 23)\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = [\n",
    "    \"id\",           # Property identifier (21,000+ unique values, not predictive)\n",
    "    \"date\",         # Original string format (replaced by date_parsed)\n",
    "    \"date_parsed\",  # Used for splitting, now replaced by days_since_start\n",
    "    \"zipcode\",      # High cardinality categorical (we use lat/long instead)\n",
    "    \"yr_built\",     # Replaced by house_age\n",
    "    \"yr_renovated\", # Replaced by was_renovated, years_since_renovation\n",
    "]\n",
    "\n",
    "train_df = train_df.drop(columns=columns_to_drop)\n",
    "val_df = val_df.drop(columns=columns_to_drop)\n",
    "test_df = test_df.drop(columns=columns_to_drop)\n",
    "\n",
    "print(f\"Columns after dropping: {list(train_df.columns)}\")\n",
    "print(f\"Shape: {train_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 6. Separate Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (15129, 22), y_train: (15129,)\n",
      "X_val:   (3242, 22), y_val:   (3242,)\n",
      "X_test:  (3242, 22), y_test:  (3242,)\n"
     ]
    }
   ],
   "source": [
    "target = \"price\"\n",
    "\n",
    "X_train = train_df.drop(columns=[target])\n",
    "y_train = train_df[target]\n",
    "\n",
    "X_val = val_df.drop(columns=[target])\n",
    "y_val = val_df[target]\n",
    "\n",
    "X_test = test_df.drop(columns=[target])\n",
    "y_test = test_df[target]\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_val:   {X_val.shape}, y_val:   {y_val.shape}\")\n",
    "print(f\"X_test:  {X_test.shape}, y_test:  {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 7. Numeric Preprocessing\n",
    "\n",
    "Most machine learning algorithms work better with normalized/standardized features. We'll apply:\n",
    "\n",
    "1. **Log transformation** for highly skewed features (square footage)\n",
    "2. **Standard scaling** for all numeric features\n",
    "\n",
    "> **⚠️ CRITICAL: Fit on Training Only**\n",
    ">\n",
    "> Scalers must be **fit on training data** and then applied to all splits.\n",
    "> \n",
    "> ```python\n",
    "> # CORRECT:\n",
    "> scaler.fit(X_train)           # Learn mean/std from training\n",
    "> X_train_scaled = scaler.transform(X_train)\n",
    "> X_val_scaled = scaler.transform(X_val)    # Apply training params\n",
    "> X_test_scaled = scaler.transform(X_test)  # Apply training params\n",
    "> ```\n",
    "> \n",
    "> **Wrong:**\n",
    "> ```python\n",
    "> # DON'T DO THIS!\n",
    "> X_train_scaled = scaler.fit_transform(X_train)\n",
    "> X_val_scaled = scaler.fit_transform(X_val)  # Leaks val statistics!\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log+scale features (6): ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_living15', 'sqft_lot15']\n",
      "Scale only (14): ['bedrooms', 'bathrooms', 'floors', 'view', 'condition', 'grade', 'lat', 'long', 'days_since_start', 'house_age', 'years_since_renovation', 'basement_ratio', 'living_vs_neighbors', 'lot_vs_neighbors']\n",
      "Passthrough (2): ['waterfront', 'was_renovated']\n"
     ]
    }
   ],
   "source": [
    "# Identify feature groups\n",
    "log_features = [\n",
    "    \"sqft_living\", \"sqft_lot\", \"sqft_above\", \"sqft_basement\", \n",
    "    \"sqft_living15\", \"sqft_lot15\"\n",
    "]\n",
    "\n",
    "# Binary features that don't need scaling\n",
    "passthrough_features = [\"waterfront\", \"was_renovated\"]\n",
    "\n",
    "# All other features get standard scaling\n",
    "scale_features = [col for col in X_train.columns \n",
    "                  if col not in log_features + passthrough_features]\n",
    "\n",
    "print(f\"Log+scale features ({len(log_features)}): {log_features}\")\n",
    "print(f\"Scale only ({len(scale_features)}): {scale_features}\")\n",
    "print(f\"Passthrough ({len(passthrough_features)}): {passthrough_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessor fitted on training data.\n"
     ]
    }
   ],
   "source": [
    "# Create preprocessing pipeline\n",
    "log_pipeline = Pipeline([\n",
    "    (\"log\", FunctionTransformer(np.log1p, validate=True, feature_names_out=\"one-to-one\")),\n",
    "    (\"scale\", StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"log\", log_pipeline, log_features),\n",
    "    (\"scale\", StandardScaler(), scale_features),\n",
    "    (\"passthrough\", \"passthrough\", passthrough_features)\n",
    "])\n",
    "\n",
    "# FIT ON TRAINING ONLY\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "print(\"Preprocessor fitted on training data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed shapes:\n",
      "  Train: (15129, 22)\n",
      "  Val:   (3242, 22)\n",
      "  Test:  (3242, 22)\n"
     ]
    }
   ],
   "source": [
    "# Transform all splits using the fitted preprocessor\n",
    "X_train_processed = preprocessor.transform(X_train)\n",
    "X_val_processed = preprocessor.transform(X_val)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"Processed shapes:\")\n",
    "print(f\"  Train: {X_train_processed.shape}\")\n",
    "print(f\"  Val:   {X_val_processed.shape}\")\n",
    "print(f\"  Test:  {X_test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features: 22\n",
      "Feature names: ['log__sqft_living', 'log__sqft_lot', 'log__sqft_above', 'log__sqft_basement', 'log__sqft_living15', 'log__sqft_lot15', 'scale__bedrooms', 'scale__bathrooms', 'scale__floors', 'scale__view', 'scale__condition', 'scale__grade', 'scale__lat', 'scale__long', 'scale__days_since_start', 'scale__house_age', 'scale__years_since_renovation', 'scale__basement_ratio', 'scale__living_vs_neighbors', 'scale__lot_vs_neighbors', 'passthrough__waterfront', 'passthrough__was_renovated']\n"
     ]
    }
   ],
   "source": [
    "# Get feature names from preprocessor\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "print(f\"Total features: {len(feature_names)}\")\n",
    "print(f\"Feature names: {list(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## 8. Verify Preprocessing\n",
    "\n",
    "Let's sanity-check our preprocessing to catch any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set statistics (should be ~0 mean, ~1 std for scaled features):\n",
      "  Means range: -0.000 to 0.045\n",
      "  Stds range:  0.088 to 1.000\n"
     ]
    }
   ],
   "source": [
    "# Check that scaling worked (training should have mean~0, std~1)\n",
    "train_means = X_train_processed.mean(axis=0)\n",
    "train_stds = X_train_processed.std(axis=0)\n",
    "\n",
    "print(\"Training set statistics (should be ~0 mean, ~1 std for scaled features):\")\n",
    "print(f\"  Means range: {train_means.min():.3f} to {train_means.max():.3f}\")\n",
    "print(f\"  Stds range:  {train_stds.min():.3f} to {train_stds.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0 NaN, 0 Inf\n",
      "Val: 0 NaN, 0 Inf\n",
      "Test: 0 NaN, 0 Inf\n",
      "\n",
      "✓ Preprocessing verification complete\n"
     ]
    }
   ],
   "source": [
    "# Check for any NaN or Inf values\n",
    "for name, data in [(\"Train\", X_train_processed), (\"Val\", X_val_processed), (\"Test\", X_test_processed)]:\n",
    "    nan_count = np.isnan(data).sum()\n",
    "    inf_count = np.isinf(data).sum()\n",
    "    print(f\"{name}: {nan_count} NaN, {inf_count} Inf\")\n",
    "    \n",
    "print(\"\\n✓ Preprocessing verification complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## 9. Save Preprocessed Data\n",
    "\n",
    "We save:\n",
    "1. Processed feature arrays and targets\n",
    "2. The fitted preprocessor (for scaling new data consistently)\n",
    "3. Reference values needed for feature engineering\n",
    "4. Feature names for interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved preprocessed data to /media/DIURNOext4/alejandro/wip-clase/PIA-SAA/examen-SAA/example_repos/king-county/processed_data\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "output_dir = Path(\"processed_data\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save processed arrays\n",
    "np.save(output_dir / \"X_train.npy\", X_train_processed)\n",
    "np.save(output_dir / \"X_val.npy\", X_val_processed)\n",
    "np.save(output_dir / \"X_test.npy\", X_test_processed)\n",
    "np.save(output_dir / \"y_train.npy\", y_train.values)\n",
    "np.save(output_dir / \"y_val.npy\", y_val.values)\n",
    "np.save(output_dir / \"y_test.npy\", y_test.values)\n",
    "\n",
    "# Save the fitted preprocessor (for scaling)\n",
    "joblib.dump(preprocessor, output_dir / \"scaler.joblib\")\n",
    "\n",
    "# Save feature engineering reference values\n",
    "reference_values = {\n",
    "    \"train_min_date\": train_min_date,\n",
    "}\n",
    "joblib.dump(reference_values, output_dir / \"reference_values.joblib\")\n",
    "\n",
    "# Save feature names\n",
    "np.save(output_dir / \"feature_names.npy\", feature_names)\n",
    "\n",
    "print(f\"Saved preprocessed data to {output_dir.absolute()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

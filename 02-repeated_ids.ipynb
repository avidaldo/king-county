{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Repeated Property IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Understanding the Data Structure\n",
    "\n",
    "Before analyzing the \"duplicates\", we must understand what each column represents:\n",
    "\n",
    "| Column | Meaning | Type |\n",
    "|--------|---------|------|\n",
    "| `id` | **Property identifier** — identifies the physical building/land | Identifier (not a feature) |\n",
    "| `date` | **Sale date** — when the transaction occurred | Temporal |\n",
    "| `price` | **Sale price** — the transaction amount | Target variable |\n",
    "| Other columns | Property characteristics | Features |\n",
    "\n",
    "**Critical distinction**: The `id` identifies **properties**, not **sales**. A property can be sold multiple times, generating multiple records with the same `id` but different `date` and `price` values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (21613, 21)\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "from pathlib import Path\n",
    "\n",
    "path = kagglehub.dataset_download(\"harlfoxem/housesalesprediction\")\n",
    "csv_path = Path(path) / \"kc_house_data.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 3. Identifying Repeated IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 21,613\n",
      "Unique property IDs: 21,436\n",
      "Properties with multiple sales: 176\n",
      "Records from repeated IDs: 353\n"
     ]
    }
   ],
   "source": [
    "id_counts = df[\"id\"].value_counts()\n",
    "repeated_ids = id_counts[id_counts > 1]\n",
    "\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Unique property IDs: {df['id'].nunique():,}\")\n",
    "print(f\"Properties with multiple sales: {len(repeated_ids):,}\")\n",
    "print(f\"Records from repeated IDs: {df['id'].isin(repeated_ids.index).sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Percentage of records with repeated IDs: 1.63%\n",
      "Percentage of properties sold more than once: 0.82%\n"
     ]
    }
   ],
   "source": [
    "# As percentages\n",
    "repeated_records = df[df[\"id\"].isin(repeated_ids.index)]\n",
    "pct_repeated_records = 100 * len(repeated_records) / len(df)\n",
    "pct_repeated_ids = 100 * len(repeated_ids) / df[\"id\"].nunique()\n",
    "\n",
    "print(f\"\\nPercentage of records with repeated IDs: {pct_repeated_records:.2f}%\")\n",
    "print(f\"Percentage of properties sold more than once: {pct_repeated_ids:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count\n",
       "1    21260\n",
       "2      175\n",
       "3        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many times are IDs repeated?\n",
    "id_counts.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 4. What Do These Repeated Records Represent?\n",
    "\n",
    "Let's examine specific examples to understand what differs between records with the same ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Property ID: 795000620\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17602</th>\n",
       "      <td>795000620</td>\n",
       "      <td>20140924T000000</td>\n",
       "      <td>115000.0</td>\n",
       "      <td>1080</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17603</th>\n",
       "      <td>795000620</td>\n",
       "      <td>20141215T000000</td>\n",
       "      <td>124000.0</td>\n",
       "      <td>1080</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17604</th>\n",
       "      <td>795000620</td>\n",
       "      <td>20150311T000000</td>\n",
       "      <td>157000.0</td>\n",
       "      <td>1080</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id             date     price  sqft_living  bedrooms  bathrooms  \\\n",
       "17602  795000620  20140924T000000  115000.0         1080         3        1.0   \n",
       "17603  795000620  20141215T000000  124000.0         1080         3        1.0   \n",
       "17604  795000620  20150311T000000  157000.0         1080         3        1.0   \n",
       "\n",
       "       grade  \n",
       "17602      5  \n",
       "17603      5  \n",
       "17604      5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Property ID: 3323059027\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6630</th>\n",
       "      <td>3323059027</td>\n",
       "      <td>20140528T000000</td>\n",
       "      <td>326000.0</td>\n",
       "      <td>1720</td>\n",
       "      <td>3</td>\n",
       "      <td>2.75</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6631</th>\n",
       "      <td>3323059027</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>340000.0</td>\n",
       "      <td>1720</td>\n",
       "      <td>3</td>\n",
       "      <td>2.75</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id             date     price  sqft_living  bedrooms  bathrooms  \\\n",
       "6630  3323059027  20140528T000000  326000.0         1720         3       2.75   \n",
       "6631  3323059027  20150225T000000  340000.0         1720         3       2.75   \n",
       "\n",
       "      grade  \n",
       "6630      7  \n",
       "6631      7  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Property ID: 1450100390\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10272</th>\n",
       "      <td>1450100390</td>\n",
       "      <td>20140905T000000</td>\n",
       "      <td>125000.0</td>\n",
       "      <td>920</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10273</th>\n",
       "      <td>1450100390</td>\n",
       "      <td>20150316T000000</td>\n",
       "      <td>208000.0</td>\n",
       "      <td>920</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id             date     price  sqft_living  bedrooms  \\\n",
       "10272  1450100390  20140905T000000  125000.0          920         3   \n",
       "10273  1450100390  20150316T000000  208000.0          920         3   \n",
       "\n",
       "       bathrooms  grade  \n",
       "10272        1.0      6  \n",
       "10273        1.0      6  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_ids = repeated_ids.head(3).index.tolist()\n",
    "\n",
    "for prop_id in example_ids:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Property ID: {prop_id}\")\n",
    "    print(\"=\"*70)\n",
    "    records = df[df[\"id\"] == prop_id].sort_values(\"date\")\n",
    "    display(records[[\"id\", \"date\", \"price\", \"sqft_living\", \"bedrooms\", \"bathrooms\", \"grade\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### 4.1 Analysis: What Changes Between Records?\n",
    "\n",
    "For properties sold multiple times, we expect:\n",
    "- **`date`**: Always different (different transaction dates)\n",
    "- **`price`**: Usually different (market conditions, negotiation)\n",
    "- **Physical features**: Mostly unchanged (same building)\n",
    "\n",
    "Let's verify this systematically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Properties where features changed between sales: 0 / 176\n",
      "\n",
      "Features that changed (and how often):\n"
     ]
    }
   ],
   "source": [
    "# Analyze what columns change between sales of the same property\n",
    "feature_cols = [col for col in df.columns if col not in [\"id\", \"date\", \"price\"]]\n",
    "\n",
    "properties_with_changes = 0\n",
    "changed_features_count = {col: 0 for col in feature_cols}\n",
    "\n",
    "for prop_id in repeated_ids.index:\n",
    "    records = df[df[\"id\"] == prop_id]\n",
    "    \n",
    "    has_any_change = False\n",
    "    for col in feature_cols:\n",
    "        if records[col].nunique() > 1:\n",
    "            changed_features_count[col] += 1\n",
    "            has_any_change = True\n",
    "    \n",
    "    if has_any_change:\n",
    "        properties_with_changes += 1\n",
    "\n",
    "print(f\"Properties where features changed between sales: {properties_with_changes} / {len(repeated_ids)}\")\n",
    "print(f\"\\nFeatures that changed (and how often):\")\n",
    "for col, count in sorted(changed_features_count.items(), key=lambda x: -x[1]):\n",
    "    if count > 0:\n",
    "        print(f\"  {col}: {count} properties\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### 4.2 Conclusion: These Are Distinct Sale Transactions\n",
    "\n",
    "The analysis confirms:\n",
    "\n",
    "1. **Same property, different sales**: Each record represents a legitimate real estate transaction\n",
    "2. Physical features remain mostly constant. Changes in features between sales represent renovations or updates, which are valid signal for the model.\n",
    "3. **Price and date differ**: These are the transaction-specific values\n",
    "\n",
    "**These are NOT duplicates in the data quality sense** — they are valid observations of different sale events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 5. The `id` Column: Why It Must Be Dropped\n",
    "\n",
    "Regardless of repeated IDs, the `id` column **must be dropped** as a feature. Here's why:\n",
    "\n",
    "### 5.1 Conceptual Reason: Identifiers Are Not Features\n",
    "\n",
    "The `id` is a **property identifier** — an arbitrary label assigned to each building. It carries no intrinsic information about the property's value. Using it as a feature would be like using a person's social security number to predict their salary.\n",
    "\n",
    "### 5.2 Technical Reason: Encoding Is Ineffective and prone to overfitting\n",
    "\n",
    "With ~21,000 unique IDs:\n",
    "- **One-hot encoding**: Would create ~21,000 binary features — computationally prohibitive and leads to extreme overfitting\n",
    "- **Label encoding**: Would impose an artificial ordinal relationship (property 1000 is not \"greater than\" property 500)\n",
    "- **Target encoding**: Would cause severe data leakage (encoding the ID with the target we're trying to predict)\n",
    "\n",
    "**Advanced Alternatives (and why they fail here):**\n",
    "In high-cardinality scenarios (e.g., Zip Codes), we often use advanced techniques. However, they require multiple examples per category to work:\n",
    "- **Leave-One-Out (LOO) Encoding**: Calculates the mean target of *other* records with the same ID. Since most IDs appear only once, LOO would either return the global mean (no signal) or, for the few repeated IDs, perfectly leak the price of the *only* other sale, leading to massive overfitting.\n",
    "- **Hash / CatBoost Encoding**: These also rely on recurrence. With `id` being almost unique per row, these techniques cannot extract a statistical pattern.\n",
    "\n",
    "### 5.3 Practical Reason: The Model Would Memorize\n",
    "\n",
    "If we included `id` as a feature, it creates a dangerous \"shortcut\" for the model:\n",
    "- **Memorization vs. Learning**: The model can learn to identify specific houses (e.g., \"if id == 123456...\") instead of learning general rules (e.g., \"if sqft > 2000...\").\n",
    "- **The \"New ID\" Problem**: While the model *might* still use other features, the ID rules become useless or harmful for any property not seen in the training set.\n",
    "- **Result**: We get a model that performs great on the training data (by memorizing IDs) but fails on new data (where those IDs don't exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current number of features: 19\n",
      "Number of unique IDs: 21,436\n",
      "\n",
      "If we one-hot encoded 'id':\n",
      "  New feature count: 21,454\n",
      "  Features would outnumber samples!\n",
      "\n",
      "This is clearly not a viable approach.\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate why ID can't be encoded\n",
    "n_unique_ids = df[\"id\"].nunique()\n",
    "n_features = df.shape[1] - 2  # excluding id and price\n",
    "\n",
    "print(f\"Current number of features: {n_features}\")\n",
    "print(f\"Number of unique IDs: {n_unique_ids:,}\")\n",
    "print(f\"\\nIf we one-hot encoded 'id':\")\n",
    "print(f\"  New feature count: {n_features + n_unique_ids - 1:,}\")\n",
    "print(f\"  Features would outnumber samples!\")\n",
    "print(f\"\\nThis is clearly not a viable approach.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 6. Implications for Model Training\n",
    "\n",
    "Now that we understand the data, let's consider the implications for model training.\n",
    "\n",
    "### 6.1 The Non-Problem: Property ID \"Leakage\"\n",
    "\n",
    "Some might worry: \"If the same property appears in train and test sets, isn't that data leakage?\"\n",
    "\n",
    "**Answer: No, provided we respect time.**\n",
    "\n",
    "If a property sold in 2014 (train) and again in 2015 (test), this is **not leakage**. In a real-world production scenario, when predicting the 2015 price, we *would* have access to the 2014 sale history. This mimics the actual information available at inference time.\n",
    "\n",
    "**Note on Identification:**\n",
    "While we drop the `id` column to prevent explicit memorization, the model might still \"identify\" properties via high-precision spatial coordinates (`lat`, `long`). This is called **spatial overfitting**, but it is distinct from the data leakage concern of using future information to predict the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 21,613\n",
      "Unique property IDs: 21,436\n",
      "Unique feature combinations: 21,420\n",
      "\n",
      "Difference: 16 properties share features with others\n"
     ]
    }
   ],
   "source": [
    "# Can properties be uniquely identified by their features alone?\n",
    "feature_cols_for_id = [\"sqft_living\", \"sqft_lot\", \"bedrooms\", \"bathrooms\", \n",
    "                       \"floors\", \"waterfront\", \"view\", \"condition\", \"grade\",\n",
    "                       \"yr_built\", \"lat\", \"long\"]\n",
    "\n",
    "# Count unique feature combinations\n",
    "unique_combinations = df.drop_duplicates(subset=feature_cols_for_id)\n",
    "\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Unique property IDs: {df['id'].nunique():,}\")\n",
    "print(f\"Unique feature combinations: {len(unique_combinations):,}\")\n",
    "print(f\"\\nDifference: {df['id'].nunique() - len(unique_combinations):,} properties share features with others\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### 6.2 The Real Problem: Temporal Leakage\n",
    "\n",
    "The actual concern with this dataset is **temporal leakage**, which affects ALL records, not just repeated IDs.\n",
    "\n",
    "**The problem**: A random train/test split mixes sales from different time periods. The model can learn from May 2015 sales to predict January 2014 prices — using \"future\" information to predict the \"past\".\n",
    "\n",
    "**The solution**: Use a **temporal split** — train on older sales, test on newer sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 7. Should We Keep All Records?\n",
    "\n",
    "Given that repeated IDs represent valid, distinct sales, should we keep all records?\n",
    "\n",
    "### Arguments FOR Keeping All Records\n",
    "\n",
    "1. **Each is a valid observation**: Different sales at different times with different prices\n",
    "2. **Richer training data**: More examples for the model to learn from\n",
    "3. **Captures market dynamics**: Price changes for the same property reflect market trends\n",
    "\n",
    "### Arguments FOR Deduplication (and why they are mostly invalid here)\n",
    "\n",
    "*Why might someone suggest keeping only the most recent record?*\n",
    "\n",
    "1.  **Feature Collision (Same X, Different Y)**:\n",
    "    *   *The Argument*: If physical features are static, the model sees the same house with two different prices.\n",
    "    *   *The Flaw*: This is only true if we ignore **Time**. Since `date` is a crucial feature, the input $X$ (House + Date) is actually unique. The price difference is explained by the time difference.\n",
    "\n",
    "2.  **Primacy of Recency**:\n",
    "    *   *The Argument*: The most recent sale is the \"true\" current value.\n",
    "    *   *The Flaw*: This applies to *appraisal* (what is it worth *now*?), but not necessarily to *machine learning training*. Old data helps the model learn how prices evolve over time. Unless the market has fundamentally broken/changed structure (e.g., pre- vs. post-housing crash), historical data is valuable signal, not noise.\n",
    "\n",
    "### Recommendation\n",
    "\n",
    "**Keep all records.**\n",
    "\n",
    "For this specific dataset, deduplication is **suboptimal** and not a sound strategy.\n",
    "\n",
    "1.  **The \"Noise\" is Signal**: The fact that the *same* house sold for different prices at different times forces the model to use the `date` feature to explain the difference. This helps the model learn market trends (inflation/deflation).\n",
    "2.  **Short Time Horizon**: This dataset spans only 1 year. The \"older\" sale is not obsolete history; it's a recent data point that helps establish the price baseline.\n",
    "3.  **Realistic Production Scenario**: If a property was sold in the training period AND the test period, this is realistic — we would actually know about the earlier sale when predicting the later one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal split (80/20):\n",
      "  Train: 17,290 records, 17,203 unique properties\n",
      "  Test: 4,323 records, 4,323 unique properties\n",
      "\n",
      "Properties appearing in BOTH sets: 90\n",
      "\n",
      "This is REALISTIC: we know about past sales when predicting future ones.\n"
     ]
    }
   ],
   "source": [
    "# With temporal split: how many properties appear in both train and test?\n",
    "df[\"date_parsed\"] = pd.to_datetime(df[\"date\"].str[:8], format=\"%Y%m%d\")\n",
    "df_sorted = df.sort_values(\"date_parsed\")\n",
    "\n",
    "split_idx = int(len(df_sorted) * 0.8)\n",
    "train_df = df_sorted.iloc[:split_idx]\n",
    "test_df = df_sorted.iloc[split_idx:]\n",
    "\n",
    "train_ids = set(train_df[\"id\"])\n",
    "test_ids = set(test_df[\"id\"])\n",
    "shared_ids = train_ids.intersection(test_ids)\n",
    "\n",
    "print(f\"Temporal split (80/20):\")\n",
    "print(f\"  Train: {len(train_df):,} records, {len(train_ids):,} unique properties\")\n",
    "print(f\"  Test: {len(test_df):,} records, {len(test_ids):,} unique properties\")\n",
    "print(f\"\\nProperties appearing in BOTH sets: {len(shared_ids)}\")\n",
    "print(f\"\\nThis is REALISTIC: we know about past sales when predicting future ones.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 8. Summary and Recommendations\n",
    "\n",
    "### What We Found\n",
    "\n",
    "| Observation | Explanation |\n",
    "|-------------|-------------|\n",
    "| ~0.85% of records have repeated IDs | Same property sold multiple times |\n",
    "| Physical features don't change | Expected — same building |\n",
    "| Price and date differ | Different transactions |\n",
    "\n",
    "### Recommended Actions\n",
    "\n",
    "| Action | Rationale |\n",
    "|--------|----------|\n",
    "| **Drop `id` column** | It's an identifier, not a feature; cannot be meaningfully encoded |\n",
    "| **Keep all records** | Each is a valid sale transaction |\n",
    "| **Use temporal split** | Prevents temporal leakage (the real issue) |\n",
    "\n",
    "### What NOT to Do\n",
    "\n",
    "| Incorrect Approach | Why It's Wrong |\n",
    "|-------------------|----------------|\n",
    "| One-hot encode `id` | Creates ~21,000 features; leads to memorization |\n",
    "| Remove \"duplicate\" records | Loses valid training data |\n",
    "| Use `GroupShuffleSplit` by `id` | Doesn't address temporal leakage; adds complexity |\n",
    "| Use random train/test split | Causes temporal leakage |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL PREPROCESSING GUIDANCE\n",
      "============================\n",
      "\n",
      "1. DROP the 'id' column:\n",
      "   df = df.drop(columns=['id'])\n",
      "\n",
      "2. KEEP all records (no deduplication needed)\n",
      "\n",
      "3. Use TEMPORAL train/test split:\n",
      "   df_sorted = df.sort_values('date')\n",
      "   split_idx = int(len(df_sorted) * 0.8)\n",
      "   train = df_sorted.iloc[:split_idx]\n",
      "   test = df_sorted.iloc[split_idx:]\n",
      "\n",
      "See notebook p3-03-temporal_leakage.ipynb for detailed explanation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "FINAL PREPROCESSING GUIDANCE\n",
    "============================\n",
    "\n",
    "1. DROP the 'id' column:\n",
    "   df = df.drop(columns=['id'])\n",
    "\n",
    "2. KEEP all records (no deduplication needed)\n",
    "\n",
    "3. Use TEMPORAL train/test split:\n",
    "   df_sorted = df.sort_values('date')\n",
    "   split_idx = int(len(df_sorted) * 0.8)\n",
    "   train = df_sorted.iloc[:split_idx]\n",
    "   test = df_sorted.iloc[split_idx:]\n",
    "\n",
    "See notebook p3-03-temporal_leakage.ipynb for detailed explanation.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## 9. Advanced Nuance: ID as a Key for Feature Engineering\n",
    "\n",
    "A critical distinction must be made regarding the `id` column. While we established that `id` should not be used as a raw feature (categorical variable) to prevent memorization, it plays a different, vital role in real-world production systems: **as a Relational Key.**\n",
    "\n",
    "### The Intuition\n",
    "In a production Automated Valuation Model (AVM), the history of a specific asset is often the strongest predictor of its current value. If a house sold for \\$500,000 last year, that is a powerful \"anchor\" for today's price.\n",
    "\n",
    "### The Implementation: Lag Features\n",
    "In a scenario with deep historical data, we would not drop `id`. Instead, we would use it to create **Lag Features**.\n",
    "1. Sort data by `id` and `date`.\n",
    "2. Shift the `price` column to create a `previous_price` feature.\n",
    "3. Calculate the delta (`price` - `previous_price`).\n",
    "4. **Then** drop the `id` column and train on the *change* in price.\n",
    "\n",
    "### The Challenge: Signal Sparsity vs. Missing Values\n",
    "You might wonder: *\"Can't modern models (like XGBoost or LightGBM) handle missing values automatically?\"*\n",
    "\n",
    "**Yes, they can.** However, the issue here is not just technical (NaNs causing errors) but statistical (Signal-to-Noise). Let's quantify how much \"signal\" actually exists in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to use ID for Lag Features:\n",
      "---------------------------------------\n",
      "Total records:               21,613\n",
      "Records with history (Signal): 177  (0.82%)\n",
      "Records with NaN (Noise):    21,436  (99.18%)\n"
     ]
    }
   ],
   "source": [
    "# Let's attempt to engineer the \"Previous Price\" feature \n",
    "# to see if it provides enough signal for the model.\n",
    "\n",
    "# 1. Sort by ID and Date to ensure correct order\n",
    "df_lag = df.sort_values(by=['id', 'date']).copy()\n",
    "\n",
    "# 2. Create the lag feature: What was the price of this specific ID in the previous row?\n",
    "df_lag['previous_price'] = df_lag.groupby('id')['price'].shift(1)\n",
    "\n",
    "# 3. Calculate metrics\n",
    "total_rows = len(df_lag)\n",
    "rows_with_history = df_lag['previous_price'].notna().sum()\n",
    "missing_history = df_lag['previous_price'].isna().sum()\n",
    "\n",
    "print(f\"Attempting to use ID for Lag Features:\")\n",
    "print(f\"---------------------------------------\")\n",
    "print(f\"Total records:               {total_rows:,}\")\n",
    "print(f\"Records with history (Signal): {rows_with_history:,}  ({(rows_with_history/total_rows)*100:.2f}%)\")\n",
    "print(f\"Records with NaN (Noise):    {missing_history:,}  ({(missing_history/total_rows)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Conclusion on Historical Features\n",
    "\n",
    "As the calculation above shows, **99.18%** of our rows would have a `NaN` (missing value) for `previous_price`.\n",
    "\n",
    "**The Verdict:**\n",
    "1.  **Handling NaNs:** While modern models *can* handle NaNs (e.g., by sending them down a default branch in a tree), they still need a sufficient number of *non-NaN* examples to learn the underlying pattern.\n",
    "2.  **Sparsity Risk:** With only ~176 examples of \"how history predicts future price,\" the model is unlikely to learn a generalizable rule. It risks treating the feature as noise or overfitting to those specific 176 instances.\n",
    "3.  **Production Reality:** In a real company database with 10 years of sales, this number might be 50% or 60%. In that case, **you would absolutely keep this feature.**\n",
    "\n",
    "**Recommendation:** For this specific educational dataset, we drop `id` to focus on physical features. In a real-world project with denser history, using `id` to build lag features is a standard and powerful technique."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (21613, 21)\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"harlfoxem/housesalesprediction\")\n",
    "csv_path = Path(path) / \"kc_house_data.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2. Parse Date and Sort Temporally\n",
    "\n",
    "Temporal ordering is **critical** for proper train/val/test splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2014-05-02 to 2015-05-27\n"
     ]
    }
   ],
   "source": [
    "# Parse date column\n",
    "df[\"date_parsed\"] = pd.to_datetime(df[\"date\"].str[:8], format=\"%Y%m%d\")\n",
    "\n",
    "# Note: The temporal_train_val_test_split function handles sorting internally,\n",
    "# but we sort here explicitly for pedagogical clarity and to show the date range.\n",
    "df = df.sort_values(\"date_parsed\").reset_index(drop=True)\n",
    "\n",
    "print(f\"Date range: {df['date_parsed'].min().date()} to {df['date_parsed'].max().date()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 3. Temporal Train/Validation/Test Split\n",
    "\n",
    "We use a **three-way temporal split** to avoid both:\n",
    "1. **Temporal leakage** (training on future data)\n",
    "2. **Model selection leakage** (using test set for hyperparameter tuning)\n",
    "\n",
    "For full rationale, see `p3-03-temporal_leakage.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_train_val_test_split(df: pd.DataFrame, \n",
    "                                   date_column: str,\n",
    "                                   val_size: float = 0.15, \n",
    "                                   test_size: float = 0.15) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split data temporally into train, validation, and test sets.\n",
    "    \n",
    "    Data is sorted by the specified date column internally, ensuring proper temporal ordering.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Data to split (will be sorted internally by date_column).\n",
    "    date_column : str\n",
    "        Name of the column containing datetime values for temporal ordering.\n",
    "    val_size : float\n",
    "        Proportion for validation set (default 0.15).\n",
    "    test_size : float  \n",
    "        Proportion for test set (default 0.15).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple of (train_df, val_df, test_df)\n",
    "        All dataframes are sorted chronologically.\n",
    "    \"\"\"\n",
    "    # Sort by date to ensure temporal ordering\n",
    "    df_sorted = df.sort_values(date_column).reset_index(drop=True)\n",
    "    \n",
    "    n = len(df_sorted)\n",
    "    train_end = int(n * (1 - val_size - test_size))\n",
    "    val_end = int(n * (1 - test_size))\n",
    "    \n",
    "    train_df = df_sorted.iloc[:train_end].copy()\n",
    "    val_df = df_sorted.iloc[train_end:val_end].copy()\n",
    "    test_df = df_sorted.iloc[val_end:].copy()\n",
    "    \n",
    "    return train_df, val_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes:\n",
      "  Train: 15,129 records (70.0%)\n",
      "  Val:   3,242 records (15.0%)\n",
      "  Test:  3,242 records (15.0%)\n",
      "\n",
      "Date ranges:\n",
      "  Train: 2014-05-02 to 2015-01-16\n",
      "  Val:   2015-01-16 to 2015-03-26\n",
      "  Test:  2015-03-26 to 2015-05-27\n"
     ]
    }
   ],
   "source": [
    "# Apply the split\n",
    "train_df, val_df, test_df = temporal_train_val_test_split(df, date_column=\"date_parsed\", val_size=0.15, test_size=0.15)\n",
    "\n",
    "print(\"Split sizes:\")\n",
    "print(f\"  Train: {len(train_df):,} records ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Val:   {len(val_df):,} records ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Test:  {len(test_df):,} records ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "print()\n",
    "print(\"Date ranges:\")\n",
    "print(f\"  Train: {train_df['date_parsed'].min().date()} to {train_df['date_parsed'].max().date()}\")\n",
    "print(f\"  Val:   {val_df['date_parsed'].min().date()} to {val_df['date_parsed'].max().date()}\")\n",
    "print(f\"  Test:  {test_df['date_parsed'].min().date()} to {test_df['date_parsed'].max().date()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Based on EDA findings, we'll:\n",
    "1. Drop non-predictive columns\n",
    "2. Extract temporal features\n",
    "3. Create derived features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### 4.1 Drop Non-Predictive Columns\n",
    "\n",
    "- **`id`**: Property identifier with 21,000+ unique values. Cannot be one-hot encoded (dimensionality explosion). Not predictive. **Must be dropped.** See `p3-02-repeated_ids.ipynb`.\n",
    "- **`date`**: Original string format (will extract features first)\n",
    "- **`zipcode`**: High cardinality categorical. We'll use lat/long instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Feature engineering transformer for house price prediction.\n",
    "    \n",
    "    Fits reference values (min_date) on training data to avoid data leakage.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    min_date_ : Timestamp\n",
    "        Minimum date from training data, used to compute days_since_start.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        \"\"\"\n",
    "        Fit the transformer by learning reference values from training data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : DataFrame\n",
    "            Training data with 'date_parsed' column.\n",
    "        y : array-like, optional\n",
    "            Target variable (not used).\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        # Store the minimum date from training data\n",
    "        # This prevents data leakage when transforming val/test sets\n",
    "        self.min_date_ = X[\"date_parsed\"].min()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Apply feature engineering transformations.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : DataFrame\n",
    "            Data with parsed date and raw features.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        DataFrame\n",
    "            Transformed data with engineered features.\n",
    "        \"\"\"\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Temporal feature: days since first sale in TRAINING dataset\n",
    "        # Uses fitted min_date_ to ensure consistency across splits\n",
    "        X[\"days_since_start\"] = (X[\"date_parsed\"] - self.min_date_).dt.days\n",
    "        \n",
    "        # Age at sale\n",
    "        sale_year = X[\"date_parsed\"].dt.year\n",
    "        X[\"house_age\"] = sale_year - X[\"yr_built\"]\n",
    "        \n",
    "        # Was renovated (binary)\n",
    "        X[\"was_renovated\"] = (X[\"yr_renovated\"] > 0).astype(int)\n",
    "        \n",
    "        # Years since renovation (0 if never renovated)\n",
    "        X[\"years_since_renovation\"] = np.where(\n",
    "            X[\"yr_renovated\"] > 0,\n",
    "            sale_year - X[\"yr_renovated\"],\n",
    "            0\n",
    "        )\n",
    "        \n",
    "        # Basement ratio\n",
    "        X[\"basement_ratio\"] = X[\"sqft_basement\"] / X[\"sqft_living\"].replace(0, 1)\n",
    "        \n",
    "        # Living area vs neighbors\n",
    "        X[\"living_vs_neighbors\"] = X[\"sqft_living\"] / X[\"sqft_living15\"].replace(0, 1)\n",
    "        \n",
    "        # Lot area vs neighbors\n",
    "        X[\"lot_vs_neighbors\"] = X[\"sqft_lot\"] / X[\"sqft_lot15\"].replace(0, 1)\n",
    "        \n",
    "        # Drop columns\n",
    "        columns_to_drop = [\n",
    "            \"id\",           # Not predictive (property identifier)\n",
    "            \"date\",         # Original string (extracted features)\n",
    "            \"date_parsed\",  # Used for splitting only\n",
    "            \"zipcode\",      # High cardinality (using lat/long)\n",
    "            \"yr_built\",     # Replaced by house_age\n",
    "            \"yr_renovated\", # Replaced by was_renovated, years_since_renovation\n",
    "        ]\n",
    "        \n",
    "        X = X.drop(columns=columns_to_drop)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 4.2 Separate Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (15129, 22), y_train: (15129,)\n",
      "X_val:   (3242, 22), y_val:   (3242,)\n",
      "X_test:  (3242, 22), y_test:  (3242,)\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "target = \"price\"\n",
    "\n",
    "X_train = train_df.drop(columns=[target])\n",
    "y_train = train_df[target]\n",
    "\n",
    "X_val = val_df.drop(columns=[target])\n",
    "y_val = val_df[target]\n",
    "\n",
    "X_test = test_df.drop(columns=[target])\n",
    "y_test = test_df[target]\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_val:   {X_val.shape}, y_val:   {y_val.shape}\")\n",
    "print(f\"X_test:  {X_test.shape}, y_test:  {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 5. Build Complete Preprocessing Pipeline\n",
    "\n",
    "We'll create a complete scikit-learn pipeline that includes:\n",
    "1. Feature engineering (custom transformer)\n",
    "2. Log transformation for skewed features\n",
    "3. Standard scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log features (6): ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_living15', 'sqft_lot15']\n",
      "Passthrough (2): ['waterfront', 'was_renovated']\n",
      "Scale only (14): ['bedrooms', 'bathrooms', 'floors', 'view', 'condition', 'grade', 'lat', 'long', 'days_since_start', 'house_age', 'years_since_renovation', 'basement_ratio', 'living_vs_neighbors', 'lot_vs_neighbors']\n"
     ]
    }
   ],
   "source": [
    "# First, temporarily fit the feature engineer to see what features we'll have\n",
    "temp_engineer = FeatureEngineer()\n",
    "temp_engineer.fit(X_train)\n",
    "temp_X = temp_engineer.transform(X_train)\n",
    "\n",
    "# Identify feature groups from the engineered features\n",
    "log_features = [\"sqft_living\", \"sqft_lot\", \"sqft_above\", \"sqft_basement\", \n",
    "                \"sqft_living15\", \"sqft_lot15\"]\n",
    "\n",
    "# Features that should not be scaled (already on reasonable scale or binary)\n",
    "passthrough_features = [\"waterfront\", \"was_renovated\"]\n",
    "\n",
    "# All other numeric features\n",
    "scale_features = [col for col in temp_X.columns \n",
    "                  if col not in log_features + passthrough_features]\n",
    "\n",
    "print(f\"Log features ({len(log_features)}): {log_features}\")\n",
    "print(f\"Passthrough ({len(passthrough_features)}): {passthrough_features}\")\n",
    "print(f\"Scale only ({len(scale_features)}): {scale_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transform pipeline\n",
    "log_pipeline = Pipeline([\n",
    "    (\"log\", FunctionTransformer(np.log1p, validate=True, feature_names_out=\"one-to-one\")), \n",
    "    (\"scale\", StandardScaler())\n",
    "])\n",
    "\n",
    "# Numeric preprocessing (log+scale, scale only, and passthrough)\n",
    "numeric_preprocessor = ColumnTransformer([\n",
    "    (\"log\", log_pipeline, log_features),\n",
    "    (\"scale\", StandardScaler(), scale_features),\n",
    "    (\"passthrough\", \"passthrough\", passthrough_features)\n",
    "])\n",
    "\n",
    "# Complete pipeline: feature engineering → numeric preprocessing\n",
    "full_pipeline = Pipeline([\n",
    "    (\"feature_engineering\", FeatureEngineer()),\n",
    "    (\"preprocessing\", numeric_preprocessor)\n",
    "])\n",
    "\n",
    "print(\"Complete pipeline structure:\")\n",
    "print(full_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "The `feature_names_out=\"one-to-one\"` parameter tells sklearn that the transformer produces the same number of output features as input features with the same names. This enables get_feature_names_out() to work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed shapes: Train=(15129, 22), Val=(3242, 22), Test=(3242, 22)\n"
     ]
    }
   ],
   "source": [
    "# Fit the complete pipeline on training data ONLY\n",
    "# This learns: min_date, scaling parameters, etc.\n",
    "full_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Transform all sets using the fitted pipeline\n",
    "X_train_processed = full_pipeline.transform(X_train)\n",
    "X_val_processed = full_pipeline.transform(X_val)\n",
    "X_test_processed = full_pipeline.transform(X_test)\n",
    "\n",
    "print(f\"Processed shapes: Train={X_train_processed.shape}, Val={X_val_processed.shape}, Test={X_test_processed.shape}\")\n",
    "print(f\"\\nFitted min_date from training: {full_pipeline.named_steps['feature_engineering'].min_date_.date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### 5.1 Verify Feature Names After Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features: 22\n",
      "Feature names: ['log__sqft_living', 'log__sqft_lot', 'log__sqft_above', 'log__sqft_basement', 'log__sqft_living15', 'log__sqft_lot15', 'scale__bedrooms', 'scale__bathrooms', 'scale__floors', 'scale__view', 'scale__condition', 'scale__grade', 'scale__lat', 'scale__long', 'scale__days_since_start', 'scale__house_age', 'scale__years_since_renovation', 'scale__basement_ratio', 'scale__living_vs_neighbors', 'scale__lot_vs_neighbors', 'passthrough__waterfront', 'passthrough__was_renovated']\n"
     ]
    }
   ],
   "source": [
    "# Get feature names from the numeric preprocessing step (after feature engineering)\n",
    "feature_names = full_pipeline.named_steps['preprocessing'].get_feature_names_out()\n",
    "print(f\"Total features: {len(feature_names)}\")\n",
    "print(f\"Feature names: {list(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 6. Save Processed Data\n",
    "\n",
    "Save for use in the modeling notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed data to /media/NOCTURNOEXTRA/Alejandro/wip-clase/PIA-SAA/example_repos/king-county/processed_data\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "output_dir = Path(\"processed_data\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save processed arrays\n",
    "np.save(output_dir / \"X_train.npy\", X_train_processed)\n",
    "np.save(output_dir / \"X_val.npy\", X_val_processed)\n",
    "np.save(output_dir / \"X_test.npy\", X_test_processed)\n",
    "np.save(output_dir / \"y_train.npy\", y_train.values)\n",
    "np.save(output_dir / \"y_val.npy\", y_val.values)\n",
    "np.save(output_dir / \"y_test.npy\", y_test.values)\n",
    "\n",
    "# Save COMPLETE pipeline for inference (includes feature engineering + preprocessing)\n",
    "joblib.dump(full_pipeline, output_dir / \"preprocessor.joblib\")\n",
    "\n",
    "# Save feature names\n",
    "np.save(output_dir / \"feature_names.npy\", feature_names)\n",
    "\n",
    "print(f\"Saved processed data to {output_dir.absolute()}\")\n",
    "print(f\"\\nSaved pipeline can now transform raw data (with date_parsed) end-to-end.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "### What was done\n",
    "\n",
    "1. **Temporal split** (70% train / 15% val / 15% test)\n",
    "   - Train: oldest data\n",
    "   - Validation: middle period (for model selection)\n",
    "   - Test: most recent (for final evaluation only)\n",
    "\n",
    "2. **Feature engineering** (as sklearn transformer)\n",
    "   - **Custom `FeatureEngineer` transformer** ensures consistency across splits\n",
    "   - Fits `min_date_` on training data (prevents data leakage)\n",
    "   - Creates `days_since_start` using training reference\n",
    "   - Creates derived features (house_age, was_renovated, ratios, etc.)\n",
    "   - Drops non-predictive columns (id, zipcode, etc.)\n",
    "\n",
    "3. **Complete sklearn pipeline**\n",
    "   - **Stage 1**: Feature engineering (custom transformer)\n",
    "   - **Stage 2**: Log + scale for square footage features\n",
    "   - **Stage 3**: Standard scaling for other numeric\n",
    "   - **Stage 4**: Passthrough for binary features\n",
    "\n",
    "### Files saved\n",
    "\n",
    "```\n",
    "processed_data/\n",
    "├── X_train.npy\n",
    "├── X_val.npy\n",
    "├── X_test.npy\n",
    "├── y_train.npy\n",
    "├── y_val.npy\n",
    "├── y_test.npy\n",
    "├── preprocessor.joblib  # COMPLETE pipeline (feature eng + preprocessing)\n",
    "└── feature_names.npy\n",
    "```\n",
    "\n",
    "### Key improvements\n",
    "\n",
    "- ✅ **No data leakage**: `min_date_` fitted on training data only\n",
    "- ✅ **Reproducible inference**: Complete pipeline saved, can transform raw data\n",
    "- ✅ **sklearn best practices**: All transformations encapsulated in pipeline\n",
    "- ✅ **Coherent design**: fit() on train, transform() on all splits\n",
    "\n",
    "### Next steps\n",
    "\n",
    "Continue to modeling notebook for model training and evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wip-clase (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

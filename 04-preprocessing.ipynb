{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (21613, 21)\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"harlfoxem/housesalesprediction\")\n",
    "csv_path = Path(path) / \"kc_house_data.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2. Parse Date and Sort Temporally\n",
    "\n",
    "Temporal ordering is **critical** for proper train/val/test splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2014-05-02 to 2015-05-27\n"
     ]
    }
   ],
   "source": [
    "# Parse date column\n",
    "df[\"date_parsed\"] = pd.to_datetime(df[\"date\"].str[:8], format=\"%Y%m%d\")\n",
    "\n",
    "# Sort by date\n",
    "df = df.sort_values(\"date_parsed\").reset_index(drop=True)\n",
    "\n",
    "print(f\"Date range: {df['date_parsed'].min().date()} to {df['date_parsed'].max().date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 3. Temporal Train/Validation/Test Split\n",
    "\n",
    "We use a **three-way temporal split** to avoid both:\n",
    "1. **Temporal leakage** (training on future data)\n",
    "2. **Model selection leakage** (using test set for hyperparameter tuning)\n",
    "\n",
    "For full rationale, see `p3-03-temporal_leakage.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_train_val_test_split(df: pd.DataFrame, \n",
    "                                   val_size: float = 0.15, \n",
    "                                   test_size: float = 0.15) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split data temporally into train, validation, and test sets.\n",
    "    \n",
    "    Data MUST be sorted by date before calling this function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Data sorted chronologically.\n",
    "    val_size : float\n",
    "        Proportion for validation set (default 0.15).\n",
    "    test_size : float  \n",
    "        Proportion for test set (default 0.15).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple of (train_df, val_df, test_df)\n",
    "    \"\"\"\n",
    "    n = len(df)\n",
    "    train_end = int(n * (1 - val_size - test_size))\n",
    "    val_end = int(n * (1 - test_size))\n",
    "    \n",
    "    train_df = df.iloc[:train_end].copy()\n",
    "    val_df = df.iloc[train_end:val_end].copy()\n",
    "    test_df = df.iloc[val_end:].copy()\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes:\n",
      "  Train: 15,129 records (70.0%)\n",
      "  Val:   3,242 records (15.0%)\n",
      "  Test:  3,242 records (15.0%)\n",
      "\n",
      "Date ranges:\n",
      "  Train: 2014-05-02 to 2015-01-16\n",
      "  Val:   2015-01-16 to 2015-03-26\n",
      "  Test:  2015-03-26 to 2015-05-27\n"
     ]
    }
   ],
   "source": [
    "# Apply the split\n",
    "train_df, val_df, test_df = temporal_train_val_test_split(df, val_size=0.15, test_size=0.15)\n",
    "\n",
    "print(\"Split sizes:\")\n",
    "print(f\"  Train: {len(train_df):,} records ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Val:   {len(val_df):,} records ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Test:  {len(test_df):,} records ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "print()\n",
    "print(\"Date ranges:\")\n",
    "print(f\"  Train: {train_df['date_parsed'].min().date()} to {train_df['date_parsed'].max().date()}\")\n",
    "print(f\"  Val:   {val_df['date_parsed'].min().date()} to {val_df['date_parsed'].max().date()}\")\n",
    "print(f\"  Test:  {test_df['date_parsed'].min().date()} to {test_df['date_parsed'].max().date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Based on EDA findings, we'll:\n",
    "1. Drop non-predictive columns\n",
    "2. Extract temporal features\n",
    "3. Create derived features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### 4.1 Drop Non-Predictive Columns\n",
    "\n",
    "- **`id`**: Property identifier with 21,000+ unique values. Cannot be one-hot encoded (dimensionality explosion). Not predictive. **Must be dropped.** See `p3-02-repeated_ids.ipynb`.\n",
    "- **`date`**: Original string format (will extract features first)\n",
    "- **`zipcode`**: High cardinality categorical. We'll use lat/long instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply feature engineering transformations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Raw data with parsed date.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame with engineered features\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Temporal feature: days since first sale in dataset\n",
    "    # This captures the temporal trend (market appreciation/depreciation)\n",
    "    # better than discrete year/month features, especially given our\n",
    "    # dataset spans only ~1 year (insufficient for seasonality estimation)\n",
    "    min_date = df[\"date_parsed\"].min()\n",
    "    df[\"days_since_start\"] = (df[\"date_parsed\"] - min_date).dt.days\n",
    "    \n",
    "    # Age at sale\n",
    "    sale_year = df[\"date_parsed\"].dt.year\n",
    "    df[\"house_age\"] = sale_year - df[\"yr_built\"]\n",
    "    \n",
    "    # Was renovated (binary)\n",
    "    df[\"was_renovated\"] = (df[\"yr_renovated\"] > 0).astype(int)\n",
    "    \n",
    "    # Years since renovation (0 if never renovated)\n",
    "    df[\"years_since_renovation\"] = np.where(\n",
    "        df[\"yr_renovated\"] > 0,\n",
    "        sale_year - df[\"yr_renovated\"],\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    # Basement ratio\n",
    "    df[\"basement_ratio\"] = df[\"sqft_basement\"] / df[\"sqft_living\"].replace(0, 1)\n",
    "    \n",
    "    # Living area vs neighbors\n",
    "    df[\"living_vs_neighbors\"] = df[\"sqft_living\"] / df[\"sqft_living15\"].replace(0, 1)\n",
    "    \n",
    "    # Lot area vs neighbors\n",
    "    df[\"lot_vs_neighbors\"] = df[\"sqft_lot\"] / df[\"sqft_lot15\"].replace(0, 1)\n",
    "    \n",
    "    # Drop columns\n",
    "    columns_to_drop = [\n",
    "        \"id\",           # Not predictive (property identifier)\n",
    "        \"date\",         # Original string (extracted features)\n",
    "        \"date_parsed\",  # Used for splitting only\n",
    "        \"zipcode\",      # High cardinality (using lat/long)\n",
    "        \"yr_built\",     # Replaced by house_age\n",
    "        \"yr_renovated\", # Replaced by was_renovated, years_since_renovation\n",
    "    ]\n",
    "    \n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineered features: 23 columns\n",
      "Columns: ['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'lat', 'long', 'sqft_living15', 'sqft_lot15', 'days_since_start', 'house_age', 'was_renovated', 'years_since_renovation', 'basement_ratio', 'living_vs_neighbors', 'lot_vs_neighbors']\n"
     ]
    }
   ],
   "source": [
    "# Apply to all sets\n",
    "train_eng = engineer_features(train_df)\n",
    "val_eng = engineer_features(val_df)\n",
    "test_eng = engineer_features(test_df)\n",
    "\n",
    "print(f\"Engineered features: {train_eng.shape[1]} columns\")\n",
    "print(f\"Columns: {list(train_eng.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### 4.2 Separate Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (15129, 22), y_train: (15129,)\n",
      "X_val:   (3242, 22), y_val:   (3242,)\n",
      "X_test:  (3242, 22), y_test:  (3242,)\n"
     ]
    }
   ],
   "source": [
    "target = \"price\"\n",
    "\n",
    "X_train = train_eng.drop(columns=[target])\n",
    "y_train = train_eng[target]\n",
    "\n",
    "X_val = val_eng.drop(columns=[target])\n",
    "y_val = val_eng[target]\n",
    "\n",
    "X_test = test_eng.drop(columns=[target])\n",
    "y_test = test_eng[target]\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_val:   {X_val.shape}, y_val:   {y_val.shape}\")\n",
    "print(f\"X_test:  {X_test.shape}, y_test:  {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 5. Build Preprocessing Pipeline\n",
    "\n",
    "We'll create a scikit-learn pipeline for:\n",
    "1. Log transformation for skewed features\n",
    "2. Standard scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log features (6): ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_living15', 'sqft_lot15']\n",
      "Passthrough (2): ['waterfront', 'was_renovated']\n",
      "Scale only (14): ['bedrooms', 'bathrooms', 'floors', 'view', 'condition', 'grade', 'lat', 'long', 'days_since_start', 'house_age', 'years_since_renovation', 'basement_ratio', 'living_vs_neighbors', 'lot_vs_neighbors']\n"
     ]
    }
   ],
   "source": [
    "# Identify feature groups\n",
    "log_features = [\"sqft_living\", \"sqft_lot\", \"sqft_above\", \"sqft_basement\", \n",
    "                \"sqft_living15\", \"sqft_lot15\"]\n",
    "\n",
    "# Features that should not be scaled (already on reasonable scale or binary)\n",
    "passthrough_features = [\"waterfront\", \"was_renovated\"]\n",
    "\n",
    "# All other numeric features\n",
    "scale_features = [col for col in X_train.columns \n",
    "                  if col not in log_features + passthrough_features]\n",
    "\n",
    "print(f\"Log features ({len(log_features)}): {log_features}\")\n",
    "print(f\"Passthrough ({len(passthrough_features)}): {passthrough_features}\")\n",
    "print(f\"Scale only ({len(scale_features)}): {scale_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transform pipeline\n",
    "log_pipeline = Pipeline([\n",
    "    (\"log\", FunctionTransformer(np.log1p, validate=True, feature_names_out=\"one-to-one\")), \n",
    "    (\"scale\", StandardScaler())\n",
    "])\n",
    "\n",
    "# Full preprocessing\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"log\", log_pipeline, log_features),\n",
    "    (\"scale\", StandardScaler(), scale_features),\n",
    "    (\"passthrough\", \"passthrough\", passthrough_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "The `feature_names_out=\"one-to-one\"` parameter tells sklearn that the transformer produces the same number of output features as input features with the same names. This enables get_feature_names_out() to work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed shapes: Train=(15129, 22), Val=(3242, 22), Test=(3242, 22)\n"
     ]
    }
   ],
   "source": [
    "# Fit on training data ONLY\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Transform all sets\n",
    "X_train_processed = preprocessor.transform(X_train)\n",
    "X_val_processed = preprocessor.transform(X_val)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"Processed shapes: Train={X_train_processed.shape}, Val={X_val_processed.shape}, Test={X_test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### 5.1 Get Feature Names After Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features: 22\n",
      "Feature names: ['log__sqft_living', 'log__sqft_lot', 'log__sqft_above', 'log__sqft_basement', 'log__sqft_living15', 'log__sqft_lot15', 'scale__bedrooms', 'scale__bathrooms', 'scale__floors', 'scale__view', 'scale__condition', 'scale__grade', 'scale__lat', 'scale__long', 'scale__days_since_start', 'scale__house_age', 'scale__years_since_renovation', 'scale__basement_ratio', 'scale__living_vs_neighbors', 'scale__lot_vs_neighbors', 'passthrough__waterfront', 'passthrough__was_renovated']\n"
     ]
    }
   ],
   "source": [
    "feature_names = preprocessor.get_feature_names_out()\n",
    "print(f\"Total features: {len(feature_names)}\")\n",
    "print(f\"Feature names: {list(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## 6. Save Processed Data\n",
    "\n",
    "Save for use in the modeling notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed data to /media/DIURNOext4/alejandro/wip-clase/PIA-SAA/projects/provisional/p3-king-county/processed_data\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "output_dir = Path(\"processed_data\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save processed arrays\n",
    "np.save(output_dir / \"X_train.npy\", X_train_processed)\n",
    "np.save(output_dir / \"X_val.npy\", X_val_processed)\n",
    "np.save(output_dir / \"X_test.npy\", X_test_processed)\n",
    "np.save(output_dir / \"y_train.npy\", y_train.values)\n",
    "np.save(output_dir / \"y_val.npy\", y_val.values)\n",
    "np.save(output_dir / \"y_test.npy\", y_test.values)\n",
    "\n",
    "# Save preprocessor for inference\n",
    "joblib.dump(preprocessor, output_dir / \"preprocessor.joblib\")\n",
    "\n",
    "# Save feature names\n",
    "np.save(output_dir / \"feature_names.npy\", feature_names)\n",
    "\n",
    "print(f\"Saved processed data to {output_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "### What was done\n",
    "\n",
    "1. **Temporal split** (70% train / 15% val / 15% test)\n",
    "   - Train: oldest data\n",
    "   - Validation: middle period (for model selection)\n",
    "   - Test: most recent (for final evaluation only)\n",
    "\n",
    "2. **Feature engineering**\n",
    "   - Created `days_since_start` to capture temporal trend (see EDA for rationale)\n",
    "   - Created derived features (house_age, was_renovated, etc.)\n",
    "   - Dropped `id` (not predictive — see p3-02)\n",
    "   - Dropped `zipcode` (using lat/long instead)\n",
    "\n",
    "3. **Preprocessing pipeline**\n",
    "   - Log + scale for square footage features\n",
    "   - Standard scaling for other numeric\n",
    "   - Passthrough for binary features\n",
    "\n",
    "### Files saved\n",
    "\n",
    "```\n",
    "processed_data/\n",
    "├── X_train.npy\n",
    "├── X_val.npy\n",
    "├── X_test.npy\n",
    "├── y_train.npy\n",
    "├── y_val.npy\n",
    "├── y_test.npy\n",
    "├── preprocessor.joblib\n",
    "└── feature_names.npy\n",
    "```\n",
    "\n",
    "### Next steps\n",
    "\n",
    "Continue to **p3-05-modeling.ipynb** for model training and evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

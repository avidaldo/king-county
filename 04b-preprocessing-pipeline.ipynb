{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Preprocessing: Production Pipeline\n",
    "\n",
    "This notebook creates a **complete sklearn pipeline** for preprocessing the King County house price dataset. The pipeline encapsulates all transformations, enabling:\n",
    "\n",
    "1. **End-to-end inference**: Transform raw CSV data directly\n",
    "2. **Reproducibility**: All fitted parameters saved in a single artifact\n",
    "3. **No data leakage**: Proper fit/transform separation enforced by design\n",
    "\n",
    "> **Prerequisite:** For step-by-step explanations of each preprocessing step, see `04a-preprocessing-step-by-step.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (21613, 21)\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"harlfoxem/housesalesprediction\")\n",
    "csv_path = Path(path) / \"kc_house_data.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Parse dates for splitting\n",
    "\n",
    "We parse dates here to enable temporal splitting. The pipeline will handle its own date parsing for end-to-end inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2014-05-02 to 2015-05-27\n"
     ]
    }
   ],
   "source": [
    "df[\"date_parsed\"] = pd.to_datetime(df[\"date\"].str[:8], format=\"%Y%m%d\")\n",
    "df = df.sort_values(\"date_parsed\").reset_index(drop=True)\n",
    "print(f\"Date range: {df['date_parsed'].min().date()} to {df['date_parsed'].max().date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Temporal split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 15,129 | Val: 3,242 | Test: 3,242\n",
      "Train dates: 2014-05-02 to 2015-01-16\n",
      "Val dates:   2015-01-16 to 2015-03-26\n",
      "Test dates:  2015-03-26 to 2015-05-27\n"
     ]
    }
   ],
   "source": [
    "def temporal_train_val_test_split(\n",
    "    df: pd.DataFrame, \n",
    "    date_column: str,\n",
    "    val_size: float = 0.15, \n",
    "    test_size: float = 0.15\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Split data temporally into train, validation, and test sets.\"\"\"\n",
    "    df_sorted = df.sort_values(date_column).reset_index(drop=True)\n",
    "    n = len(df_sorted)\n",
    "    train_end = int(n * (1 - val_size - test_size))\n",
    "    val_end = int(n * (1 - test_size))\n",
    "    return (\n",
    "        df_sorted.iloc[:train_end].copy(),\n",
    "        df_sorted.iloc[train_end:val_end].copy(),\n",
    "        df_sorted.iloc[val_end:].copy()\n",
    "    )\n",
    "\n",
    "train_df, val_df, test_df = temporal_train_val_test_split(df, date_column=\"date_parsed\")\n",
    "\n",
    "print(f\"Train: {len(train_df):,} | Val: {len(val_df):,} | Test: {len(test_df):,}\")\n",
    "print(f\"Train dates: {train_df['date_parsed'].min().date()} to {train_df['date_parsed'].max().date()}\")\n",
    "print(f\"Val dates:   {val_df['date_parsed'].min().date()} to {val_df['date_parsed'].max().date()}\")\n",
    "print(f\"Test dates:  {test_df['date_parsed'].min().date()} to {test_df['date_parsed'].max().date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 2. Custom Transformers\n",
    "\n",
    "We create custom sklearn transformers to encapsulate all preprocessing logic. Each transformer follows the sklearn protocol:\n",
    "\n",
    "- `fit(X, y=None)`: Learn parameters from training data only\n",
    "- `transform(X)`: Apply transformation using learned parameters\n",
    "- Fitted parameters end with underscore (e.g., `min_date_`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### 2.1 DateParser\n",
    "\n",
    "Transforms the raw date string into a datetime column. This enables true end-to-end inference from CSV data.\n",
    "\n",
    "This is a **stateless** transformer (no fitting needed) — it just parses strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DateParser(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Parse date strings to datetime.\n",
    "    \n",
    "    Converts the 'date' column from format \"YYYYMMDDTHHMMSS\" to datetime.\n",
    "    This is a stateless transformer (no fitting needed).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    date_column : str\n",
    "        Name of the column containing date strings (default: \"date\").\n",
    "    output_column : str\n",
    "        Name for the parsed datetime column (default: \"date_parsed\").\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, date_column: str = \"date\", output_column: str = \"date_parsed\"):\n",
    "        self.date_column = date_column\n",
    "        self.output_column = output_column\n",
    "    \n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        \"\"\"No fitting needed — returns self.\"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Parse date strings to datetime.\"\"\"\n",
    "        X = X.copy()\n",
    "        X[self.output_column] = pd.to_datetime(\n",
    "            X[self.date_column].str[:8], \n",
    "            format=\"%Y%m%d\"\n",
    "        )\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### 2.2 FeatureEngineer\n",
    "\n",
    "Creates all derived features and drops non-predictive columns.\n",
    "\n",
    "> **Key design:** The `min_date_` is learned during `fit()` from training data, then used in `transform()` for all data. This prevents data leakage in the `days_since_start` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Feature engineering transformer for house price prediction.\n",
    "    \n",
    "    Creates derived features and drops non-predictive columns.\n",
    "    \n",
    "    Fitted Parameters\n",
    "    -----------------\n",
    "    min_date_ : Timestamp\n",
    "        Minimum date from training data. Used to compute days_since_start\n",
    "        consistently across all splits (preventing data leakage).\n",
    "    \n",
    "    Features Created\n",
    "    ----------------\n",
    "    - days_since_start: Days since min_date_ (temporal trend)\n",
    "    - house_age: Age of house at sale time\n",
    "    - was_renovated: Binary indicator\n",
    "    - years_since_renovation: Years since last renovation\n",
    "    - basement_ratio: sqft_basement / sqft_living\n",
    "    - living_vs_neighbors: sqft_living / sqft_living15\n",
    "    - lot_vs_neighbors: sqft_lot / sqft_lot15\n",
    "    \n",
    "    Columns Dropped\n",
    "    ---------------\n",
    "    id, date, date_parsed, zipcode, yr_built, yr_renovated\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        \"\"\"\n",
    "        Learn reference values from training data.\n",
    "        \n",
    "        Stores min_date_ to ensure consistent days_since_start across splits.\n",
    "        \"\"\"\n",
    "        # Validate required columns\n",
    "        required = [\"date_parsed\", \"yr_built\", \"yr_renovated\", \"sqft_basement\", \n",
    "                    \"sqft_living\", \"sqft_living15\", \"sqft_lot\", \"sqft_lot15\"]\n",
    "        missing = set(required) - set(X.columns)\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing required columns: {missing}\")\n",
    "        \n",
    "        # Store minimum date from TRAINING data\n",
    "        # This prevents data leakage when transforming val/test\n",
    "        self.min_date_ = X[\"date_parsed\"].min()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Apply feature engineering transformations.\n",
    "        \n",
    "        Uses fitted min_date_ for days_since_start calculation.\n",
    "        \"\"\"\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Temporal feature: days since training start\n",
    "        # Uses fitted min_date_ (NOT recomputed from X)\n",
    "        X[\"days_since_start\"] = (X[\"date_parsed\"] - self.min_date_).dt.days\n",
    "        \n",
    "        # Age features\n",
    "        sale_year = X[\"date_parsed\"].dt.year\n",
    "        X[\"house_age\"] = sale_year - X[\"yr_built\"]\n",
    "        X[\"was_renovated\"] = (X[\"yr_renovated\"] > 0).astype(int)\n",
    "        X[\"years_since_renovation\"] = np.where(\n",
    "            X[\"yr_renovated\"] > 0,\n",
    "            sale_year - X[\"yr_renovated\"],\n",
    "            0\n",
    "        )\n",
    "        \n",
    "        # Ratio features\n",
    "        X[\"basement_ratio\"] = X[\"sqft_basement\"] / X[\"sqft_living\"].replace(0, 1)\n",
    "        X[\"living_vs_neighbors\"] = X[\"sqft_living\"] / X[\"sqft_living15\"].replace(0, 1)\n",
    "        X[\"lot_vs_neighbors\"] = X[\"sqft_lot\"] / X[\"sqft_lot15\"].replace(0, 1)\n",
    "        \n",
    "        # Drop non-predictive columns\n",
    "        columns_to_drop = [\n",
    "            \"id\",           # Property identifier\n",
    "            \"date\",         # Original string format\n",
    "            \"date_parsed\",  # Used for feature engineering only\n",
    "            \"zipcode\",      # High cardinality (using lat/long)\n",
    "            \"yr_built\",     # Replaced by house_age\n",
    "            \"yr_renovated\", # Replaced by was_renovated, years_since_renovation\n",
    "        ]\n",
    "        X = X.drop(columns=columns_to_drop)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 3. Build Complete Pipeline\n",
    "\n",
    "The pipeline has three stages:\n",
    "\n",
    "1. **DateParser**: Raw date string → datetime\n",
    "2. **FeatureEngineer**: Create derived features, drop non-predictive columns\n",
    "3. **NumericPreprocessor**: Log transform + scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log+scale (6): ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_living15', 'sqft_lot15']\n",
      "Scale only (14): ['bedrooms', 'bathrooms', 'floors', 'view', 'condition', 'grade', 'lat', 'long', 'days_since_start', 'house_age', 'years_since_renovation', 'basement_ratio', 'living_vs_neighbors', 'lot_vs_neighbors']\n",
      "Passthrough (2): ['waterfront', 'was_renovated']\n"
     ]
    }
   ],
   "source": [
    "# Define feature groups (after feature engineering)\n",
    "log_features = [\n",
    "    \"sqft_living\", \"sqft_lot\", \"sqft_above\", \"sqft_basement\",\n",
    "    \"sqft_living15\", \"sqft_lot15\"\n",
    "]\n",
    "\n",
    "passthrough_features = [\"waterfront\", \"was_renovated\"]\n",
    "\n",
    "# We need to know all features after FeatureEngineer to define scale_features\n",
    "# First, get a sample output from FeatureEngineer\n",
    "temp_engineer = Pipeline([\n",
    "    (\"date_parser\", DateParser()),\n",
    "    (\"feature_engineer\", FeatureEngineer())\n",
    "])\n",
    "\n",
    "# Separate X/y for fitting\n",
    "X_train_raw = train_df.drop(columns=[\"price\", \"date_parsed\"])  # Raw features (keep 'date' for pipeline)\n",
    "y_train = train_df[\"price\"]\n",
    "\n",
    "# Get sample output to determine all feature names\n",
    "temp_engineer.fit(X_train_raw)\n",
    "temp_output = temp_engineer.transform(X_train_raw)\n",
    "\n",
    "all_features = list(temp_output.columns)\n",
    "scale_features = [f for f in all_features if f not in log_features + passthrough_features]\n",
    "\n",
    "print(f\"Log+scale ({len(log_features)}): {log_features}\")\n",
    "print(f\"Scale only ({len(scale_features)}): {scale_features}\")\n",
    "print(f\"Passthrough ({len(passthrough_features)}): {passthrough_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete pipeline structure:\n",
      "Pipeline(steps=[('date_parser', DateParser()),\n",
      "                ('feature_engineer', FeatureEngineer()),\n",
      "                ('preprocessing',\n",
      "                 ColumnTransformer(transformers=[('log',\n",
      "                                                  Pipeline(steps=[('log',\n",
      "                                                                   FunctionTransformer(feature_names_out='one-to-one',\n",
      "                                                                                       func=<ufunc 'log1p'>,\n",
      "                                                                                       validate=True)),\n",
      "                                                                  ('scale',\n",
      "                                                                   StandardScaler())]),\n",
      "                                                  ['sqft_living', 'sqft_lot',\n",
      "                                                   'sqft_above',\n",
      "                                                   'sqft_basement',\n",
      "                                                   'sqft_living15',\n",
      "                                                   'sqft_lot15']),\n",
      "                                                 ('scale', StandardScaler(),\n",
      "                                                  ['bedrooms', 'bathrooms',\n",
      "                                                   'floors', 'view',\n",
      "                                                   'condition', 'grade', 'lat',\n",
      "                                                   'long', 'days_since_start',\n",
      "                                                   'house_age',\n",
      "                                                   'years_since_renovation',\n",
      "                                                   'basement_ratio',\n",
      "                                                   'living_vs_neighbors',\n",
      "                                                   'lot_vs_neighbors']),\n",
      "                                                 ('passthrough', 'passthrough',\n",
      "                                                  ['waterfront',\n",
      "                                                   'was_renovated'])]))])\n"
     ]
    }
   ],
   "source": [
    "# Log transformation + scaling pipeline\n",
    "log_pipeline = Pipeline([\n",
    "    (\"log\", FunctionTransformer(np.log1p, validate=True, feature_names_out=\"one-to-one\")),\n",
    "    (\"scale\", StandardScaler())\n",
    "])\n",
    "\n",
    "# Numeric preprocessor (applied after feature engineering)\n",
    "numeric_preprocessor = ColumnTransformer([\n",
    "    (\"log\", log_pipeline, log_features),\n",
    "    (\"scale\", StandardScaler(), scale_features),\n",
    "    (\"passthrough\", \"passthrough\", passthrough_features)\n",
    "])\n",
    "\n",
    "# COMPLETE PIPELINE: Raw data → Processed features\n",
    "full_pipeline = Pipeline([\n",
    "    (\"date_parser\", DateParser()),\n",
    "    (\"feature_engineer\", FeatureEngineer()),\n",
    "    (\"preprocessing\", numeric_preprocessor)\n",
    "])\n",
    "\n",
    "print(\"Complete pipeline structure:\")\n",
    "print(full_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 4. Fit and Transform\n",
    "\n",
    "We fit the complete pipeline on training data, then transform all splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw X shapes: Train=(15129, 20), Val=(3242, 20), Test=(3242, 20)\n"
     ]
    }
   ],
   "source": [
    "# Prepare raw X (without date_parsed, but with date string)\n",
    "X_train = train_df.drop(columns=[\"price\", \"date_parsed\"])\n",
    "y_train = train_df[\"price\"]\n",
    "\n",
    "X_val = val_df.drop(columns=[\"price\", \"date_parsed\"])\n",
    "y_val = val_df[\"price\"]\n",
    "\n",
    "X_test = test_df.drop(columns=[\"price\", \"date_parsed\"])\n",
    "y_test = test_df[\"price\"]\n",
    "\n",
    "print(f\"Raw X shapes: Train={X_train.shape}, Val={X_val.shape}, Test={X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted min_date from training: 2014-05-02\n"
     ]
    }
   ],
   "source": [
    "# FIT on training data only\n",
    "full_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Show fitted parameters\n",
    "fitted_min_date = full_pipeline.named_steps['feature_engineer'].min_date_\n",
    "print(f\"Fitted min_date from training: {fitted_min_date.date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed shapes:\n",
      "  Train: (15129, 22)\n",
      "  Val:   (3242, 22)\n",
      "  Test:  (3242, 22)\n"
     ]
    }
   ],
   "source": [
    "# Transform all splits\n",
    "X_train_processed = full_pipeline.transform(X_train)\n",
    "X_val_processed = full_pipeline.transform(X_val)\n",
    "X_test_processed = full_pipeline.transform(X_test)\n",
    "\n",
    "print(f\"Processed shapes:\")\n",
    "print(f\"  Train: {X_train_processed.shape}\")\n",
    "print(f\"  Val:   {X_val_processed.shape}\")\n",
    "print(f\"  Test:  {X_test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total features: 22\n",
      "Feature names: ['log__sqft_living', 'log__sqft_lot', 'log__sqft_above', 'log__sqft_basement', 'log__sqft_living15', 'log__sqft_lot15', 'scale__bedrooms', 'scale__bathrooms', 'scale__floors', 'scale__view', 'scale__condition', 'scale__grade', 'scale__lat', 'scale__long', 'scale__days_since_start', 'scale__house_age', 'scale__years_since_renovation', 'scale__basement_ratio', 'scale__living_vs_neighbors', 'scale__lot_vs_neighbors', 'passthrough__waterfront', 'passthrough__was_renovated']\n"
     ]
    }
   ],
   "source": [
    "# Get feature names\n",
    "feature_names = full_pipeline.named_steps['preprocessing'].get_feature_names_out()\n",
    "print(f\"\\nTotal features: {len(feature_names)}\")\n",
    "print(f\"Feature names: {list(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 5. Pipeline Validation\n",
    "\n",
    "We verify that the pipeline correctly prevents data leakage and works end-to-end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### 5.1 Verify `days_since_start` consistency\n",
    "\n",
    "The `days_since_start` feature should have **increasing ranges** across splits (since they're ordered temporally). If each split starts from 0, we have data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "days_since_start (SCALED) ranges:\n",
      "  Train: -1.67 to 1.99\n",
      "  Val:   1.99 to 2.97\n",
      "  Test:  2.97 to 3.85\n",
      "\n",
      "✓ No data leakage detected: temporal ordering preserved\n"
     ]
    }
   ],
   "source": [
    "# Find the index of days_since_start in the final features\n",
    "days_idx = list(feature_names).index('scale__days_since_start')\n",
    "\n",
    "train_days = X_train_processed[:, days_idx]\n",
    "val_days = X_val_processed[:, days_idx]\n",
    "test_days = X_test_processed[:, days_idx]\n",
    "\n",
    "print(\"days_since_start (SCALED) ranges:\")\n",
    "print(f\"  Train: {train_days.min():.2f} to {train_days.max():.2f}\")\n",
    "print(f\"  Val:   {val_days.min():.2f} to {val_days.max():.2f}\")\n",
    "print(f\"  Test:  {test_days.min():.2f} to {test_days.max():.2f}\")\n",
    "\n",
    "# Validation: val min should be > train min (it's later in time)\n",
    "assert val_days.min() > train_days.min(), \"Data leakage detected in validation set!\"\n",
    "assert test_days.min() > val_days.min(), \"Data leakage detected in test set!\"\n",
    "print(\"\\n✓ No data leakage detected: temporal ordering preserved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### 5.2 Test end-to-end inference\n",
    "\n",
    "The pipeline should be able to transform completely raw data (as it would come from a CSV) without any preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw input data:\n",
      "           date  sqft_living  yr_built\n",
      "20150701T000000         1800      1990\n"
     ]
    }
   ],
   "source": [
    "# Create a synthetic \"new\" row as it would come from raw CSV\n",
    "new_data = pd.DataFrame([{\n",
    "    \"id\": 9999999,\n",
    "    \"date\": \"20150701T000000\",  # Raw date string!\n",
    "    \"bedrooms\": 3,\n",
    "    \"bathrooms\": 2.0,\n",
    "    \"sqft_living\": 1800,\n",
    "    \"sqft_lot\": 5000,\n",
    "    \"floors\": 1.0,\n",
    "    \"waterfront\": 0,\n",
    "    \"view\": 0,\n",
    "    \"condition\": 3,\n",
    "    \"grade\": 7,\n",
    "    \"sqft_above\": 1800,\n",
    "    \"sqft_basement\": 0,\n",
    "    \"yr_built\": 1990,\n",
    "    \"yr_renovated\": 0,\n",
    "    \"zipcode\": 98001,\n",
    "    \"lat\": 47.5,\n",
    "    \"long\": -122.2,\n",
    "    \"sqft_living15\": 1750,\n",
    "    \"sqft_lot15\": 4800\n",
    "}])\n",
    "\n",
    "print(\"Raw input data:\")\n",
    "print(new_data[[\"date\", \"sqft_living\", \"yr_built\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed shape: (1, 22)\n",
      "Processed values (first 5 features):\n",
      "  log__sqft_living: -0.1486\n",
      "  log__sqft_lot: -0.5295\n",
      "  log__sqft_above: 0.2176\n",
      "  log__sqft_basement: -0.8008\n",
      "  log__sqft_living15: -0.2343\n",
      "\n",
      "✓ End-to-end inference works!\n"
     ]
    }
   ],
   "source": [
    "# Transform using the complete pipeline\n",
    "new_processed = full_pipeline.transform(new_data)\n",
    "\n",
    "print(f\"Processed shape: {new_processed.shape}\")\n",
    "print(f\"Processed values (first 5 features):\")\n",
    "for i, name in enumerate(feature_names[:5]):\n",
    "    print(f\"  {name}: {new_processed[0, i]:.4f}\")\n",
    "\n",
    "print(\"\\n✓ End-to-end inference works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## 6. Save Pipeline and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /media/DIURNOext4/alejandro/wip-clase/PIA-SAA/examen-SAA/example_repos/king-county/processed_data\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "output_dir = Path(\"processed_data\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save processed arrays\n",
    "np.save(output_dir / \"X_train.npy\", X_train_processed)\n",
    "np.save(output_dir / \"X_val.npy\", X_val_processed)\n",
    "np.save(output_dir / \"X_test.npy\", X_test_processed)\n",
    "np.save(output_dir / \"y_train.npy\", y_train.values)\n",
    "np.save(output_dir / \"y_val.npy\", y_val.values)\n",
    "np.save(output_dir / \"y_test.npy\", y_test.values)\n",
    "\n",
    "# Save the COMPLETE pipeline (includes everything!)\n",
    "joblib.dump(full_pipeline, output_dir / \"preprocessor.joblib\")\n",
    "\n",
    "# Save feature names\n",
    "np.save(output_dir / \"feature_names.npy\", feature_names)\n",
    "\n",
    "print(f\"Saved to {output_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## 7. Inference Example\n",
    "\n",
    "This section shows how to use the saved pipeline for inference on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pipeline min_date: 2014-05-02\n"
     ]
    }
   ],
   "source": [
    "# Simulate loading in a new session\n",
    "loaded_pipeline = joblib.load(output_dir / \"preprocessor.joblib\")\n",
    "\n",
    "# Check that fitted parameters are preserved\n",
    "print(f\"Loaded pipeline min_date: {loaded_pipeline.named_steps['feature_engineer'].min_date_.date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference result shape: (1, 22)\n",
      "\n",
      "✓ Ready for model prediction!\n"
     ]
    }
   ],
   "source": [
    "# Process new data using loaded pipeline\n",
    "new_data_inference = pd.DataFrame([{\n",
    "    \"id\": 8888888,\n",
    "    \"date\": \"20151015T000000\",\n",
    "    \"bedrooms\": 4,\n",
    "    \"bathrooms\": 2.5,\n",
    "    \"sqft_living\": 2500,\n",
    "    \"sqft_lot\": 8000,\n",
    "    \"floors\": 2.0,\n",
    "    \"waterfront\": 0,\n",
    "    \"view\": 1,\n",
    "    \"condition\": 4,\n",
    "    \"grade\": 8,\n",
    "    \"sqft_above\": 2000,\n",
    "    \"sqft_basement\": 500,\n",
    "    \"yr_built\": 2005,\n",
    "    \"yr_renovated\": 0,\n",
    "    \"zipcode\": 98052,\n",
    "    \"lat\": 47.6,\n",
    "    \"long\": -122.1,\n",
    "    \"sqft_living15\": 2400,\n",
    "    \"sqft_lot15\": 7500\n",
    "}])\n",
    "\n",
    "X_new = loaded_pipeline.transform(new_data_inference)\n",
    "print(f\"Inference result shape: {X_new.shape}\")\n",
    "print(\"\\n✓ Ready for model prediction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Pipeline contents\n",
    "\n",
    "The saved `preprocessor.joblib` contains:\n",
    "\n",
    "| Stage | Transformer | Fitted Parameters |\n",
    "|-------|-------------|-------------------|\n",
    "| 1 | DateParser | None (stateless) |\n",
    "| 2 | FeatureEngineer | `min_date_` (from training) |\n",
    "| 3 | ColumnTransformer | Scaler means/stds |\n",
    "\n",
    "### How to use for inference\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Load pipeline\n",
    "pipeline = joblib.load(\"processed_data/preprocessor.joblib\")\n",
    "\n",
    "# Load raw CSV data\n",
    "new_data = pd.read_csv(\"new_houses.csv\")\n",
    "\n",
    "# Transform (handles everything automatically)\n",
    "X = pipeline.transform(new_data)\n",
    "\n",
    "# Predict (requires trained model)\n",
    "model = joblib.load(\"model.joblib\")\n",
    "predictions = model.predict(X)\n",
    "```\n",
    "\n",
    "### Key benefits\n",
    "\n",
    "- ✅ **No data leakage**: `min_date_` learned from training only\n",
    "- ✅ **End-to-end inference**: Accepts raw CSV data with date strings\n",
    "- ✅ **Single artifact**: All preprocessing in one `joblib` file\n",
    "- ✅ **sklearn compatible**: Works with `GridSearchCV`, `cross_val_score`, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
